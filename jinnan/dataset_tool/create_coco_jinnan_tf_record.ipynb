{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#utf-8\n",
    "import hashlib\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import contextlib2\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import random\n",
    "import sys\n",
    "sys.path.append('/Users/hywel/Documents/GitSrc/models/research')\n",
    "\n",
    "from object_detection.dataset_tools import tf_record_creation_util\n",
    "from object_detection.utils import dataset_util\n",
    "from object_detection.utils import label_map_util\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xe4\\xb8\\xad\\xe5\\x9b\\xbd'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"中国\"\n",
    "a.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "铁壳打火机\n"
     ]
    }
   ],
   "source": [
    "name = r\"\\u94c1\\u58f3\\u6253\\u706b\\u673a\"\n",
    "print(name.encode('ascii').decode('unicode_escape'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_tf_example(image,\n",
    "                      annotations_list,\n",
    "                      image_dir,\n",
    "                      category_index,\n",
    "                      include_masks=False):\n",
    "    \"\"\"Converts image and annotations to a tf.Example proto.\n",
    "\n",
    "    Args:\n",
    "      image: dict with keys:\n",
    "        [u'license', u'file_name', u'coco_url', u'height', u'width',\n",
    "        u'date_captured', u'flickr_url', u'id']\n",
    "      annotations_list:\n",
    "        list of dicts with keys:\n",
    "        [u'segmentation', u'area', u'iscrowd', u'image_id',\n",
    "        u'bbox', u'category_id', u'id']\n",
    "        Notice that bounding box coordinates in the official COCO dataset are\n",
    "        given as [x, y, width, height] tuples using absolute coordinates where\n",
    "        x, y represent the top-left (0-indexed) corner.  This function converts\n",
    "        to the format expected by the Tensorflow Object Detection API (which is\n",
    "        which is [ymin, xmin, ymax, xmax] with coordinates normalized relative\n",
    "        to image size).\n",
    "      image_dir: directory containing the image files.\n",
    "      category_index: a dict containing COCO category information keyed\n",
    "        by the 'id' field of each category.  See the\n",
    "        label_map_util.create_category_index function.\n",
    "      include_masks: Whether to include instance segmentations masks\n",
    "        (PNG encoded) in the result. default: False.\n",
    "    Returns:\n",
    "      example: The converted tf.Example\n",
    "      num_annotations_skipped: Number of (invalid) annotations that were ignored.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: if the image pointed to by data['filename'] is not a valid JPEG\n",
    "    \"\"\"\n",
    "    image_height = image['height']\n",
    "    image_width = image['width']\n",
    "    filename = image['file_name']\n",
    "    image_id = image['id']\n",
    "\n",
    "    full_path = os.path.join(image_dir, filename)\n",
    "    with tf.gfile.GFile(full_path, 'rb') as fid:\n",
    "        encoded_jpg = fid.read()\n",
    "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
    "    image = PIL.Image.open(encoded_jpg_io)\n",
    "    key = hashlib.sha256(encoded_jpg).hexdigest()\n",
    "\n",
    "    xmin = []\n",
    "    xmax = []\n",
    "    ymin = []\n",
    "    ymax = []\n",
    "    is_crowd = []\n",
    "    category_names = []\n",
    "    category_ids = []\n",
    "    area = []\n",
    "    encoded_mask_png = []\n",
    "    num_annotations_skipped = 0\n",
    "    for object_annotations in annotations_list:\n",
    "        (x, y, width, height) = tuple(object_annotations['bbox'])\n",
    "        if width <= 0 or height <= 0:\n",
    "            num_annotations_skipped += 1\n",
    "            continue\n",
    "        if x + width > image_width or y + height > image_height:\n",
    "            num_annotations_skipped += 1\n",
    "            continue\n",
    "        xmin.append(float(x) / image_width)\n",
    "        xmax.append(float(x + width) / image_width)\n",
    "        ymin.append(float(y) / image_height)\n",
    "        ymax.append(float(y + height) / image_height)\n",
    "        is_crowd.append(object_annotations['iscrowd'])\n",
    "        category_id = int(object_annotations['category_id'])\n",
    "        category_ids.append(category_id)\n",
    "        category_names.append(\n",
    "            category_index[category_id]['name'].encode('utf8'))\n",
    "        area.append(object_annotations['area'])\n",
    "\n",
    "        if include_masks:\n",
    "            run_len_encoding = mask.frPyObjects(object_annotations['segmentation'],\n",
    "                                                image_height, image_width)\n",
    "            binary_mask = mask.decode(run_len_encoding)\n",
    "            if not object_annotations['iscrowd']:\n",
    "                binary_mask = np.amax(binary_mask, axis=2)\n",
    "            pil_image = PIL.Image.fromarray(binary_mask)\n",
    "            output_io = io.BytesIO()\n",
    "            pil_image.save(output_io, format='PNG')\n",
    "            encoded_mask_png.append(output_io.getvalue())\n",
    "    feature_dict = {\n",
    "        'image/height':\n",
    "            dataset_util.int64_feature(image_height),\n",
    "        'image/width':\n",
    "            dataset_util.int64_feature(image_width),\n",
    "        'image/filename':\n",
    "            dataset_util.bytes_feature(filename.encode('utf8')),\n",
    "        'image/source_id':\n",
    "            dataset_util.bytes_feature(str(image_id).encode('utf8')),\n",
    "        'image/key/sha256':\n",
    "            dataset_util.bytes_feature(key.encode('utf8')),\n",
    "        'image/encoded':\n",
    "            dataset_util.bytes_feature(encoded_jpg),\n",
    "        'image/format':\n",
    "            dataset_util.bytes_feature('jpeg'.encode('utf8')),\n",
    "        'image/object/bbox/xmin':\n",
    "            dataset_util.float_list_feature(xmin),\n",
    "        'image/object/bbox/xmax':\n",
    "            dataset_util.float_list_feature(xmax),\n",
    "        'image/object/bbox/ymin':\n",
    "            dataset_util.float_list_feature(ymin),\n",
    "        'image/object/bbox/ymax':\n",
    "            dataset_util.float_list_feature(ymax),\n",
    "        'image/object/class/text':\n",
    "            dataset_util.bytes_list_feature(category_names),\n",
    "        'image/object/is_crowd':\n",
    "            dataset_util.int64_list_feature(is_crowd),\n",
    "#         'image/object/area':\n",
    "#             dataset_util.float_list_feature(area),\n",
    "    }\n",
    "    if include_masks:\n",
    "        feature_dict['image/object/mask'] = (\n",
    "            dataset_util.bytes_list_feature(encoded_mask_png))\n",
    "    example = tf.train.Example(\n",
    "        features=tf.train.Features(feature=feature_dict))\n",
    "    return key, example, num_annotations_skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_normal_tf_example(image,\n",
    "                      image_dir,\n",
    "                      category_index):\n",
    "    \"\"\"Converts image and annotations to a tf.Example proto.\n",
    "\n",
    "    Args:\n",
    "      image: dict with keys:\n",
    "        [u'license', u'file_name', u'coco_url', u'height', u'width',\n",
    "        u'date_captured', u'flickr_url', u'id']\n",
    "      image_dir: directory containing the image files.\n",
    "      category_index: a dict containing COCO category information keyed\n",
    "        by the 'id' field of each category.  See the\n",
    "        label_map_util.create_category_index function.\n",
    "    Returns:\n",
    "      example: The converted tf.Example\n",
    "      num_annotations_skipped: Number of (invalid) annotations that were ignored.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: if the image pointed to by data['filename'] is not a valid JPEG\n",
    "    \"\"\"\n",
    "    image_height = image['height']\n",
    "    image_width = image['width']\n",
    "    filename = image['file_name']\n",
    "    image_id = image['id']\n",
    "\n",
    "    full_path = os.path.join(image_dir, filename)\n",
    "    with tf.gfile.GFile(full_path, 'rb') as fid:\n",
    "        encoded_jpg = fid.read()\n",
    "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
    "    image = PIL.Image.open(encoded_jpg_io)\n",
    "    key = hashlib.sha256(encoded_jpg).hexdigest()\n",
    "\n",
    "    xmin = []\n",
    "    xmax = []\n",
    "    ymin = []\n",
    "    ymax = []\n",
    "    is_crowd = []\n",
    "    category_names = []\n",
    "    category_ids = []\n",
    "    area = []\n",
    "    encoded_mask_png = []\n",
    "    num_annotations_skipped = 0\n",
    "\n",
    "    feature_dict = {\n",
    "        'image/height':\n",
    "            dataset_util.int64_feature(image_height),\n",
    "        'image/width':\n",
    "            dataset_util.int64_feature(image_width),\n",
    "        'image/filename':\n",
    "            dataset_util.bytes_feature(filename.encode('utf8')),\n",
    "        'image/source_id':\n",
    "            dataset_util.bytes_feature(str(image_id).encode('utf8')),\n",
    "        'image/key/sha256':\n",
    "            dataset_util.bytes_feature(key.encode('utf8')),\n",
    "        'image/encoded':\n",
    "            dataset_util.bytes_feature(encoded_jpg),\n",
    "        'image/format':\n",
    "            dataset_util.bytes_feature('jpeg'.encode('utf8')),\n",
    "        'image/object/bbox/xmin':\n",
    "            dataset_util.float_list_feature(xmin),\n",
    "        'image/object/bbox/xmax':\n",
    "            dataset_util.float_list_feature(xmax),\n",
    "        'image/object/bbox/ymin':\n",
    "            dataset_util.float_list_feature(ymin),\n",
    "        'image/object/bbox/ymax':\n",
    "            dataset_util.float_list_feature(ymax),\n",
    "        'image/object/class/text':\n",
    "            dataset_util.bytes_list_feature(category_names),\n",
    "        'image/object/is_crowd':\n",
    "            dataset_util.int64_list_feature(is_crowd),\n",
    "#         'image/object/area':\n",
    "#             dataset_util.float_list_feature(area),\n",
    "    }\n",
    "    example = tf.train.Example(\n",
    "        features=tf.train.Features(feature=feature_dict))\n",
    "    return key, example, num_annotations_skipped\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _create_tf_record_from_coco_annotations(\n",
    "        annotations_file, image_dir, normal_image_dir, output_path, include_masks, num_shards, train):\n",
    "    \"\"\"Loads COCO annotation json files and converts to tf.Record format.\n",
    "\n",
    "    Args:\n",
    "      annotations_file: JSON file containing bounding box annotations.\n",
    "      image_dir: Directory containing the image files.\n",
    "      output_path: Path to output tf.Record file.\n",
    "      include_masks: Whether to include instance segmentations masks\n",
    "        (PNG encoded) in the result. default: False.\n",
    "      num_shards: number of output file shards.\n",
    "    \"\"\"\n",
    "    with contextlib2.ExitStack() as tf_record_close_stack, \\\n",
    "            tf.gfile.GFile(annotations_file, 'r') as fid:\n",
    "        output_tfrecords = tf_record_creation_util.open_sharded_output_tfrecords(\n",
    "            tf_record_close_stack, output_path, num_shards)\n",
    "        groundtruth_data = json.load(fid)\n",
    "        images = groundtruth_data['images']\n",
    "        category_index = label_map_util.create_category_index(\n",
    "            groundtruth_data['categories'])\n",
    "\n",
    "        annotations_index = {}\n",
    "        if 'annotations' in groundtruth_data:\n",
    "            tf.logging.info(\n",
    "                'Found groundtruth annotations. Building annotations index.')\n",
    "            for annotation in groundtruth_data['annotations']:\n",
    "                image_id = annotation['image_id']\n",
    "                if image_id not in annotations_index:\n",
    "                    annotations_index[image_id] = []\n",
    "                annotations_index[image_id].append(annotation)\n",
    "        missing_annotation_count = 0\n",
    "        for image in images:\n",
    "            image_id = image['id']\n",
    "            if image_id not in annotations_index:\n",
    "                missing_annotation_count += 1\n",
    "                annotations_index[image_id] = []\n",
    "        tf.logging.info('%d images are missing annotations.',\n",
    "                        missing_annotation_count)\n",
    "\n",
    "        total_num_annotations_skipped = 0\n",
    "        \n",
    "        image_data1 = [(image, 'restricted') for image in images]\n",
    "        \n",
    "        \n",
    "        import os \n",
    "        image_data2 = []\n",
    "        for i,path in enumerate(os.listdir(normal_image_dir)[0:300]):\n",
    "            image = {}\n",
    "            img=PIL.Image.open(normal_image_dir+'/'+path)\n",
    "            width, height = img.size\n",
    "            image['height'] = height\n",
    "            image['width'] = width\n",
    "            image['file_name'] = path\n",
    "            #large 1459\n",
    "            image['id'] = i+1460\n",
    "            image_data2 += [(image,'normal')]\n",
    " \n",
    "        image_data1 = [(image, 'restricted') for image in images]\n",
    "        \n",
    "        if train:\n",
    "            image_data = image_data1[0:1300]\n",
    "            image_data.extend(image_data2[0:200])\n",
    "\n",
    "            random.shuffle(image_data)\n",
    "        else:\n",
    "            image_data = image_data1[1300:]\n",
    "            image_data.extend(image_data2[200:])\n",
    "            random.shuffle(image_data)\n",
    "\n",
    "        for idx, image in enumerate(image_data):\n",
    "            if idx % 100 == 0:\n",
    "                tf.logging.info('On image %d of %d', idx, len(image_data))\n",
    "            if image[1] == 'restricted':\n",
    "                #print (image)\n",
    "                annotations_list = annotations_index[image[0]['id']]\n",
    "                _, tf_example, num_annotations_skipped = create_tf_example(\n",
    "                    image[0], annotations_list, image_dir, category_index, include_masks)\n",
    "                total_num_annotations_skipped += num_annotations_skipped\n",
    "            else:\n",
    "                _, tf_example, num_annotations_skipped = create_normal_tf_example(image[0],\n",
    "                      normal_image_dir,category_index)\n",
    "                \n",
    "            shard_idx = idx % num_shards\n",
    "            output_tfrecords[shard_idx].write(tf_example.SerializeToString())\n",
    "                \n",
    "        tf.logging.info('Finished writing, skipped %d annotations.',\n",
    "                        total_num_annotations_skipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found groundtruth annotations. Building annotations index.\n",
      "INFO:tensorflow:5 images are missing annotations.\n",
      "INFO:tensorflow:On image 0 of 260\n",
      "INFO:tensorflow:On image 100 of 260\n",
      "INFO:tensorflow:On image 200 of 260\n",
      "INFO:tensorflow:Finished writing, skipped 0 annotations.\n"
     ]
    }
   ],
   "source": [
    "train_annotations_file = '/Users/hywel/Downloads/jinnan2_round1_train_20190222/train_no_poly.json' \n",
    "train_image_dir = '/Users/hywel/Downloads/jinnan2_round1_train_20190222/restricted' \n",
    "normal_image_dir = '/Users/hywel/Downloads/jinnan2_round1_train_20190222/normal' \n",
    "output_dir = '/Users/hywel/Downloads/jinnan2_round1_train_20190222'\n",
    "\n",
    "train_output_path = os.path.join(output_dir, 'jinnan_val.record')\n",
    "\n",
    "_create_tf_record_from_coco_annotations(\n",
    "        train_annotations_file,\n",
    "        train_image_dir,\n",
    "        normal_image_dir,\n",
    "        train_output_path,\n",
    "        False,\n",
    "        1,\n",
    "        False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('/Users/hywel/Downloads/jinnan2_round1_train_20190222/normal')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "im = PIL.Image.open('/Users/hywel/Downloads/jinnan2_round1_train_20190222/normal/190105_091930_00150781.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "img=Image.open('/Users/hywel/Downloads/jinnan2_round1_train_20190222/normal/190105_091930_00150781.jpg')\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-4802fcebd761>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'image' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
